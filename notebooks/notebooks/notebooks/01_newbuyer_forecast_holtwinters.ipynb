This notebook contains:
- Label engineering
- Daily time series construction
- Holt-Winters forecasting for NewBuyer segment


# NewBuyer Daily Customer Forecasting

This notebook demonstrates how customer-level transaction data
is transformed into a segment-level daily time series and forecasted
using Holt-Winters (Triple Exponential Smoothing).

---

## Problem
Forecast future daily **NewBuyer** customer counts based on historical
transaction data.

---

## Label Engineering
Customers are labeled based on the day difference between consecutive orders.

```python
df["OrderDate"] = pd.to_datetime(df["OrderDate"], errors="coerce")
df = df.sort_values(["customer_email", "OrderDate"])
df["neww"] = df.groupby("customer_email")["OrderDate"].diff().dt.days

conditions = [
    df["neww"].isna(),
    df["neww"] <= 365,
    df["neww"] > 365
]
choices = ["NewBuyer", "drop", "ChurnNewBuyer"]
df["label"] = np.select(conditions, choices)

df_ts = (
    df.groupby(["label", df["OrderDate"].dt.date])["customer_email"]
      .nunique()
      .reset_index(name="customer_count")
)


---

## Time Series Preparation (Daily)

Customer-level data is aggregated into a **daily time series**
by counting unique customers per day for the NewBuyer segment.

Missing days are filled with zero to ensure a continuous daily index.

```python
# Select NewBuyer segment
nb = df_ts[df_ts["label"] == "NewBuyer"].copy()
nb["order_day"] = pd.to_datetime(nb["order_day"])

# Set daily index
nb = nb.sort_values("order_day").set_index("order_day")[["customer_count"]]
nb = nb.rename(columns={"customer_count": "y"})

# Fill missing days
full_idx = pd.date_range(nb.index.min(), nb.index.max(), freq="D")
nb = nb.reindex(full_idx)
nb.index.name = "order_day"
nb["y"] = nb["y"].fillna(0)

---

## Forecasting with Holt-Winters (Triple Exponential Smoothing)

We apply Holt-Winters with **additive trend** and **additive seasonality**.
Since the dataset spans **2+ years**, we use **365-day seasonal period**.

Because explicit campaign/event calendars are not available, we apply a **robust smoothing**
step (rolling median) to reduce the impact of unobserved spikes.

```python
from datetime import timedelta
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from sklearn.metrics import mean_absolute_error
import numpy as np

# Robust smoothing (handles unobserved event spikes)
nb["y_clean"] = nb["y"].rolling(window=7, center=True, min_periods=1).median()

# Train/Test split (last 180 days as test)
test_days = 180
split_date = nb.index.max() - timedelta(days=test_days)

train = nb[nb.index <= split_date]
test  = nb[nb.index > split_date]

seasonal_periods = 365

model = ExponentialSmoothing(
    train["y_clean"],
    trend="add",
    seasonal="add",
    seasonal_periods=seasonal_periods
).fit(optimized=True)

pred_test = model.forecast(len(test))

# Evaluation
mae = mean_absolute_error(test["y_clean"], pred_test)
mape = (np.abs(test["y_clean"] - pred_test) / np.maximum(1, np.abs(test["y_clean"]))).mean() * 100

print(f"MAE : {mae:.2f}")
print(f"MAPE: {mape:.2f}%")

future_days = 90
future_forecast = model.forecast(future_days)
future_forecast.head()


---

## Interpretation
The forecast captures both long-term trend and yearly seasonality in new customer acquisition.
Short-term fluctuations are smoothed to reduce the impact of unobserved promotional effects.

